{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to build a model for recongition of 3D printing mishaps and good prints\n",
    "\n",
    "The following code has the following setup\n",
    "A training,validation and test set from \"images\" which is from the \"printing_errors_original\" folder. This new folder is organzied by class\n",
    "A second test set from the folder \"print_errors_2_modified_for_test\" is based off our real-World set that has a large variation of photos. That could be provided by a customer\n",
    "\n",
    "This jupyter notebook focuses on the seperation of the two and building a model SOLELY on the images from the \"image\" folder\n",
    "\n",
    "THe other Jupyter notebook in this repo, will be trained in the mix of the two image datasets. to see if it will improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries that will be needed\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import ToTensor\n",
    "import random \n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 854\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import folders for train and test\n",
    "image_directory = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 0.000001\n",
    "batch = 50\n",
    "image_size = 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your image directory path\n",
    "image_directory = \"images\"\n",
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = datasets.ImageFolder(root=image_directory,\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize(image_size),\n",
    "                                          transforms.CenterCrop(image_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the train, validation, and test sizes\n",
    "train_size = int(0.7 * len(custom_dataset))  # 70% for training\n",
    "val_size = int(0.1 * len(custom_dataset))   # 10% for validation\n",
    "test_size = len(custom_dataset) - train_size - val_size  # Remaining 20% for testing\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    custom_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# PyTorch Data Initialization\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "\n",
    "# Using ngpu\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building\n",
    "\n",
    "#PyTorch\n",
    "#Second PyTorch Model\n",
    "#AlexNet Architecture\n",
    "\n",
    "class pyTorchModel_2(nn.Module):\n",
    "    def __init__(self,ngpu, num_classes=4):\n",
    "        super(pyTorchModel_2, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "pymodel1 = pyTorchModel_2(ngpu).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If only one model\n",
    "optimizer1 = torch.optim.RMSprop(pymodel1.parameters(), lr=lr )\n",
    "\n",
    "torch.use_deterministic_algorithms(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, trainLoss):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        #TrainLoss.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= num_batches\n",
    "    trainLoss.append(train_loss)\n",
    "    \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, testLoss, acc):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #testLoss.append(test_loss.item())\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    testLoss.append(test_loss)\n",
    "    acc.append((100*correct))\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def validate(model, dataloader, loss_fn, valLoss, acc):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    valLoss.append(val_loss)\n",
    "    acc.append((100 * correct))\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainLoss = []\n",
    "# testLoss = []\n",
    "# acc = []\n",
    "\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, pymodel1, loss_fn, optimizer1, trainLoss)\n",
    "#     test_loop(test_dataloader, pymodel1, loss_fn, testLoss, acc)\n",
    "\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoss = []\n",
    "testLoss = []\n",
    "valLoss = []\n",
    "acc = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, pymodel1, loss_fn, optimizer1, trainLoss)\n",
    "    validate(pymodel1, val_dataloader, loss_fn, valLoss, acc)\n",
    "\n",
    "print(\"Training and Validation Done!\")\n",
    "\n",
    "# After training, you can use the test_loop function to evaluate on a separate test set.\n",
    "test_loop(test_dataloader, pymodel1, loss_fn, testLoss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            all_predictions.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming num_classes is the number of classes in your problem (4 in this case)\n",
    "num_classes = 4\n",
    "\n",
    "def calculate_multiclass_metrics(predictions, labels, num_classes):\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(labels, predictions, labels=range(num_classes))\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    precision = precision_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "    recall = recall_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "    f1 = f1_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "\n",
    "    return conf_matrix, precision, recall, f1\n",
    "\n",
    "\n",
    "###This is used to test predictions from the dataset that is SIMILAR to the training data\n",
    "###\n",
    "###\n",
    "# Use the test_loop function to get predictions on the test set\n",
    "test_predictions, test_labels = evaluate_predictions(pymodel1, test_dataloader)\n",
    "\n",
    "# Calculate multiclass metrics\n",
    "conf_matrix, precision, recall, f1 = calculate_multiclass_metrics(test_predictions, test_labels, num_classes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPrecision:\")\n",
    "print(precision)\n",
    "print(\"\\nRecall:\")\n",
    "print(recall)\n",
    "print(\"\\nF1 Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This test is a set of photos that are not SIMILAR to the test dataset, in terms of how the images are presented\n",
    "### This is more of a real world test set, something a customer would send as photos\n",
    "# Set your image directory path\n",
    "real_image_directory = \"print_errors_2_modified_for_test\"\n",
    "\n",
    "# Create a custom dataset\n",
    "real_world_dataset = datasets.ImageFolder(root=real_image_directory,\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize(image_size),\n",
    "                                          transforms.CenterCrop(image_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ]))\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch Data Initialization\n",
    "real_world_dataloader = torch.utils.data.DataLoader(real_world_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Testing Real World\n",
    "real_world_num_classes = 3\n",
    "\n",
    "# Use the test_loop function to get predictions on the test set\n",
    "real_world_test_predictions, real_world_test_labels = evaluate_predictions(pymodel1, real_world_dataloader)\n",
    "\n",
    "# Calculate multiclass metrics\n",
    "real_world_conf_matrix, real_world_precision, real_world_recall, real_world_f1 = calculate_multiclass_metrics(real_world_test_predictions, real_world_test_labels, real_world_num_classes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(real_world_conf_matrix)\n",
    "print(\"\\nPrecision:\")\n",
    "print(real_world_precision)\n",
    "print(\"\\nRecall:\")\n",
    "print(real_world_recall)\n",
    "print(\"\\nF1 Score:\")\n",
    "print(real_world_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Loss throughout Epochs\")\n",
    "plt.plot(valLoss, label=\"val\")\n",
    "plt.plot(trainLoss, label = \"train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

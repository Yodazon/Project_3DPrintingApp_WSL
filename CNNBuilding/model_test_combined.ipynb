{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to build a model for recongition of 3D printing mishaps and good prints\n",
    "\n",
    "The following code has the following setup, it has a combiantion fo photos from \"printing_errors_original\" and \"print_errors_2_modified_for_test\"made into a folder called \"images_combined\n",
    "\n",
    "A training,validation and test set from \"images_combined\". This folder is organzied by class\n",
    "\n",
    "The goal for this model is to train on a mix of lab setting and real world photos to see if the model can perform better than the model in \"model_testing_seperated\" jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  774\n"
     ]
    }
   ],
   "source": [
    "#Import libraries that will be needed\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import ToTensor\n",
    "import random \n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 555\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'good', 1: 'spaghetti', 2: 'stringing', 3: 'underextrusion'}\n"
     ]
    }
   ],
   "source": [
    "#import folders for train and test\n",
    "image_directory = \"images_combined\"\n",
    "\n",
    "\n",
    "class_names = sorted(os.listdir(image_directory))\n",
    "class_mapping = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "lr = 0.000001\n",
    "batch = 50\n",
    "image_size = 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your image directory path\n",
    "image_directory = \"images_combined\"\n",
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = datasets.ImageFolder(root=image_directory,\n",
    "                                      transform=transforms.Compose([\n",
    "                                          transforms.Resize(image_size),\n",
    "                                          transforms.CenterCrop(image_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Modified Code, got rid of test set March 10, 2024\n",
    "###\n",
    "\n",
    "# Calculate the train, validation, and test sizes\n",
    "train_size = int(0.7 * len(custom_dataset))  # 70% for training\n",
    "val_size = int(0.1 * len(custom_dataset)) # 10% for validation\n",
    "test_size = len(custom_dataset) - train_size - val_size  # Remaining 20% for testing\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    custom_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# PyTorch Data Initialization\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "\n",
    "# Using ngpu\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building\n",
    "\n",
    "#PyTorch\n",
    "#Second PyTorch Model\n",
    "#AlexNet Architecture\n",
    "\n",
    "class pyTorchModel_2(nn.Module):\n",
    "    def __init__(self,ngpu, num_classes=4):\n",
    "        super(pyTorchModel_2, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "pymodel1 = pyTorchModel_2(ngpu).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If only one model\n",
    "optimizer1 = torch.optim.RMSprop(pymodel1.parameters(), lr=lr )\n",
    "\n",
    "torch.use_deterministic_algorithms(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, trainLoss):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        #TrainLoss.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= num_batches\n",
    "    trainLoss.append(train_loss)\n",
    "    \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, testLoss, acc):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #testLoss.append(test_loss.item())\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    testLoss.append(test_loss)\n",
    "    acc.append((100*correct))\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def validate(model, dataloader, loss_fn, valLoss, acc):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    valLoss.append(val_loss)\n",
    "    acc.append((100 * correct))\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.542802  [   51/11669]\n",
      "loss: 1.120964  [ 5151/11669]\n",
      "loss: 0.979472  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.829874 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.892903  [   51/11669]\n",
      "loss: 0.713361  [ 5151/11669]\n",
      "loss: 0.883657  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.736636 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.843636  [   51/11669]\n",
      "loss: 0.727681  [ 5151/11669]\n",
      "loss: 0.731419  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.646279 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.599193  [   51/11669]\n",
      "loss: 0.729546  [ 5151/11669]\n",
      "loss: 0.632954  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.558880 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.679208  [   51/11669]\n",
      "loss: 0.559897  [ 5151/11669]\n",
      "loss: 0.484044  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.478309 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.643639  [   51/11669]\n",
      "loss: 0.544941  [ 5151/11669]\n",
      "loss: 0.469753  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.411286 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.380572  [   51/11669]\n",
      "loss: 0.311943  [ 5151/11669]\n",
      "loss: 0.484326  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.356216 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.493066  [   51/11669]\n",
      "loss: 0.335943  [ 5151/11669]\n",
      "loss: 0.356524  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.309169 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.461636  [   51/11669]\n",
      "loss: 0.367491  [ 5151/11669]\n",
      "loss: 0.334769  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.275755 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.447785  [   51/11669]\n",
      "loss: 0.332406  [ 5151/11669]\n",
      "loss: 0.345695  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.246578 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.240286  [   51/11669]\n",
      "loss: 0.180721  [ 5151/11669]\n",
      "loss: 0.404654  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.208910 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.289928  [   51/11669]\n",
      "loss: 0.233278  [ 5151/11669]\n",
      "loss: 0.173050  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.187072 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.147175  [   51/11669]\n",
      "loss: 0.148079  [ 5151/11669]\n",
      "loss: 0.184196  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.166486 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.264598  [   51/11669]\n",
      "loss: 0.144250  [ 5151/11669]\n",
      "loss: 0.230527  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.157608 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.185966  [   51/11669]\n",
      "loss: 0.125156  [ 5151/11669]\n",
      "loss: 0.157810  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.144833 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.139797  [   51/11669]\n",
      "loss: 0.095487  [ 5151/11669]\n",
      "loss: 0.085142  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.132466 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.291410  [   51/11669]\n",
      "loss: 0.148331  [ 5151/11669]\n",
      "loss: 0.115896  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.119815 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.186339  [   51/11669]\n",
      "loss: 0.133393  [ 5151/11669]\n",
      "loss: 0.132450  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.115821 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.176940  [   51/11669]\n",
      "loss: 0.099703  [ 5151/11669]\n",
      "loss: 0.189661  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.107013 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.107305  [   51/11669]\n",
      "loss: 0.078552  [ 5151/11669]\n",
      "loss: 0.118755  [10251/11669]\n",
      "Validation Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.099792 \n",
      "\n",
      "Training and Validation Done!\n"
     ]
    }
   ],
   "source": [
    "trainLoss = []\n",
    "testLoss = []\n",
    "valLoss = []\n",
    "acc = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, pymodel1, loss_fn, optimizer1, trainLoss)\n",
    "    validate(pymodel1, val_dataloader, loss_fn, valLoss, acc)\n",
    "\n",
    "print(\"Training and Validation Done!\")\n",
    "\n",
    "# After training, you can use the test_loop function to evaluate on a separate test set.\n",
    "#test_loop(test_dataloader, pymodel1, loss_fn, testLoss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            all_predictions.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1445   16    9   17]\n",
      " [   6   54    4   11]\n",
      " [  17    5  703    0]\n",
      " [  33    0    2 1013]]\n",
      "\n",
      "Precision:\n",
      "[0.96269154 0.72       0.97910864 0.97310279]\n",
      "\n",
      "Recall:\n",
      "[0.97175521 0.72       0.96965517 0.96660305]\n",
      "\n",
      "F1 Score:\n",
      "[0.96720214 0.72       0.97435897 0.96984203]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming num_classes is the number of classes in your problem (4 in this case)\n",
    "num_classes = 4\n",
    "\n",
    "def calculate_multiclass_metrics(predictions, labels, num_classes):\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(labels, predictions, labels=range(num_classes))\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    precision = precision_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "    recall = recall_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "    f1 = f1_score(labels, predictions, average=None, labels=range(num_classes))\n",
    "\n",
    "    return conf_matrix, precision, recall, f1\n",
    "\n",
    "\n",
    "###This is used to test predictions from the dataset that is SIMILAR to the training data\n",
    "###\n",
    "###\n",
    "# Use the test_loop function to get predictions on the test set\n",
    "test_predictions, test_labels = evaluate_predictions(pymodel1, test_dataloader)\n",
    "\n",
    "# Calculate multiclass metrics\n",
    "conf_matrix, precision, recall, f1 = calculate_multiclass_metrics(test_predictions, test_labels, num_classes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nPrecision:\")\n",
    "print(precision)\n",
    "print(\"\\nRecall:\")\n",
    "print(recall)\n",
    "print(\"\\nF1 Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the image is: stringing\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_single_image(model, image_path, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Set the model to evaluation mode and make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_image = input_image.to(device)\n",
    "        output = model(input_image)\n",
    "\n",
    "    # Interpret the model output\n",
    "    predicted_index = output.argmax(1).item()\n",
    "    predicted_class_name = class_mapping[predicted_index]\n",
    "    \n",
    "\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "\n",
    "image_path = \"test_image\\\\spag.jpg\"\n",
    "predicted_label = predict_single_image(pymodel1, image_path, class_mapping)\n",
    "print(f\"The predicted label for the image is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'CNNModelV0_2.pth'\n",
    "torch.save(pymodel1.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
